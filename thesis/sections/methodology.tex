\thispagestyle{plain}
\section{Methodology}

The Danish power grid DK1 is dominated by highly volatile wind power \parencite{iea2023,wang2017}. Consequently, the forecasting task constitutes a complex time-series problem that, as documented in the literature reviewed in \autoref{sec:lit}, is characterized by pronounced nonlinearities, strong seasonality across multiple temporal scales (hourly, daily, and yearly), and exogenous drivers such as wind speed, market demand, and cross-border electricity flows \parencite{entsoe2022}, all of which interact in a non-stationary manner \parencite{hyndman2021,box2015,carlini2023}. As indicated by the same body of literature, a promising strategy is to employ a long short-term memory neural network (LSTM) architecture \parencite{leerbeck2020,bokde2021,kohut2025,ostermann2024,lowry2018}. With the theoretical foundation just presented in \autoref{sec:theory}, this section details the methodology used to address the problem statement. Recall the main research question:

\researchquestion{}

This section presents the systematic approach employed to develop and evaluate LSTMs for carbon emission forecasting in DK1. It begins by justifying the selection of LSTM architectures, followed by detailed descriptions of data collection, feature engineering, and preprocessing procedures that prepare the dataset for modeling. The section then outlines the LSTM model architectures for both 1-hour and 24-hour forecasting horizons, establishes baseline models for comparative evaluation, describes approach to feature importance analysis, and concludes with implementation details and methodological limitations.

\subsection{Justification}

The current body of literature indicate that LSTMs are a viable modeling approach for carbon emission forecasting in volatile wind power grids \parencite{leerbeck2020,bokde2021,kohut2025,ostermann2024}. This is due to several architectural advantages that align with the temporal characteristics of electrical grid dynamics:

First, carbon emissions exhibit multi-scale temporal dependencies from immediate renewable fluctuations to longer-term weather and demand patterns \parencite{bokde2021}. LSTM gating mechanisms selectively retain relevant information across these different time horizons \parencite{goodfellow2016,hochreiter1997}.

Second, wind power generation introduces significant volatility into grid operations \parencite{wang2017, iea2023}. Sudden changes in wind conditions can trigger compensatory responses from conventional generators, affecting carbon emissions with complex lag structures \parencite{carlini2023,dong2019}. The LSTM's gating mechanisms enable it to detect these wind-related events and maintain their influence in memory for an appropriate duration \parencite{goodfellow2016}. For example, if a significant drop in wind generation typically leads to increased emissions for several hours as thermal plants ramp up, the LSTM can learn to maintain this causal relationship in its cell state.

Third, carbon emissions are influenced by a combination of deterministic patterns (such as daily load cycles) and stochastic factors (such as unexpected plant outages or forecast errors in renewable generation) \parencite{leerbeck2020}. The LSTM's flexible memory management allows it to learn which patterns are predictable and which require adaptation based on recent observations. The input gate can selectively incorporate new information that deviates from expected patterns, enabling the model to adjust its predictions in response to unusual conditions \parencite{goodfellow2016}.

Finally, the forecasting task often benefits from integrating information from multiple data streams with different temporal characteristics \parencite{leerbeck2020,kohut2025}, such as scheduled generation, weather forecasts, and historical emission measurements. The LSTM's ability to maintain a rich internal representation through its cell state enables it to effectively fuse these diverse information sources and extract the most relevant signals for prediction.

\subsection{Data Collection and Sources}

The dataset used in this thesis encompasses carbon emissions and a range of explanatory variables for the DK1 grid. Observations are of hourly resolution from January 1, 2022, to January 1, 2025, resulting in approximately 26,304 (\((365+365+366)\times24\)) observations per variable. The primary source is \href{https://www.energyquantified.com/}{Energy Quantified} (EQ), a proprietary platform that offers a comprehensive suite of data on the European energy market. EQ supplies the target variable, formally labeled "DK1 Carbon Operational Emission Power Production kgCO2eq 15min Synthetic." This variable contains historical carbon emission values from electricity generation in the DK1 grid, expressed in kilograms of carbon-dioxide equivalent (kg. \cotwoe{}). The "Synthetic" designation indicates that missing observations have been imputed with EQ's proprietary models. Besides supplying the target variable, EQ also provides a set of explanatory variables as historical forecasts. Working with forecasts rather than realized values aligns the modeling exercise with the information that is available at prediction time.

First and foremost, is power consumption in the DK1 bidding zone. This variable captures the anticipated load in the grid. DK1 operates under the merit-order/economic-dispatch principle, a ruleset provided by \citeauthor{entsoe2021} (ENTSO-E) in their paper \citetitle{entsoe2021}, where electricity generation follows an economic dispatch order with lower-cost producers dispatched before higher-cost ones. Renewables with near-zero marginal costs occupy the bottom of this supply stack, while expensive thermal units are only activated as residual demand increases \parencite{roungkvist2020}. This principle directly links electricity demand, generation mix, and resulting carbon emissions across all explanatory variables.

Besides consumption are production related variables such as solar photovoltaic production, on- and offshore wind power production, and residual production day-ahead. Solar photovoltaic and wind production forecasts are particularly critical in the DK1 context, as Denmark has one of the highest wind penetration rates globally \parencite{wang2017,iea2023}. These renewable production forecasts indicate zero-marginal-cost generation availability, while residual production day-ahead represents the anticipated thermal generation gap after accounting for renewables, directly correlating with carbon emissions. Together, these production forecasts enable the LSTM models to anticipate the supply-demand balance and the resulting dispatch order, providing essential information for accurately predicting the carbon footprint of electricity generation in the DK1 grid.

Considering more market based explanatory variables are price spot day-ahead and exchange day-ahead schedule net export. The price spot day-ahead represents the forecasted electricity price in the DK1 bidding zone for the following day, determined through the market clearing process where supply and demand curves intersect. This price variable serves as a direct indicator of the marginal generation cost required to meet demand, effectively revealing which generators are expected to be dispatched under the merit-order principle. Low electricity prices typically signal that demand can be met primarily by low-cost renewable sources \parencite{sensfuss2008}, suggesting minimal carbon emissions, while high prices indicate that expensive thermal units higher up the merit-order curve must be activated, correlating with increased carbon emissions. The price thus acts as a market-derived proxy for the carbon footprint of the generation mix \parencite{wang2017}. The exchange day-ahead schedule net export captures the planned electricity flows between DK1 and neighboring regions, reflecting the interconnected nature of the European electricity market. A positive net export indicates that DK1 expects to sell electricity to adjacent areas, suggesting abundant local generation capacity, often from wind power during favorable conditions \parencite{green2012}. Conversely, negative values (net imports) signal that DK1 anticipates importing electricity to meet local demand, which can have varying carbon implications depending on the generation mix of the exporting regions. These cross-border flows are particularly relevant for carbon emission forecasting because they affect the local supply-demand balance and can either displace local thermal generation (through imports) or indicate surplus clean generation available for export. Together, these market variables provide the LSTM models with crucial information about the economic and physical constraints that drive generator dispatch decisions, enhancing the ability of the models to predict the resulting carbon emissions in the DK1 grid.

Temperature forecasts serve as weather-driven demand indicators, as heating and cooling demands directly influence electricity consumption patterns \parencite{yao2021}, particularly in Denmark's climate where heating requirements during cold periods significantly increase consumption \parencite{cassarino2018}. Lower temperatures increase demand for carbon-intensive thermal units higher in the merit-order stack \parencite{sensfuss2008}, while milder temperatures allow renewables to meet larger proportions of reduced demand \parencite{gils2014}.

The explanatory variables described above constitute the core dataset collected from external sources and serve as the foundation for the final modeling dataset. For validation purposes, the target variable was cross-checked against the public \href{https://www.energidataservice.dk/}{Energi Data Service}, maintained by the Danish transmission system operator Energinet. This cross-validation confirmed the reliability of the EQ carbon emission data. Although Energi Data Service also provides carbon emission data and some of the explanatory variables used in this thesis, the final dataset relies exclusively on EQ data to maintain consistency across all variables and avoid potential measurement discrepancies or methodological differences between data sources. Additional predictive features, particularly time-based variables that capture temporal patterns and seasonality effects, will be systematically derived from this foundational data through feature engineering techniques, as covered in the following subsection.

\subsection{Feature Engineering}

With the current set of variables, many exogenous factors are captured. However, as the literature demonstrates, fluctuations in carbon emissions can also be attributed to time-dependent features such as hour of day, weekday, month of year, and similar temporal patterns \parencite{wang2017}. Intuitively, this relationship makes sense because carbon emissions are fundamentally linked to human behavioral patterns and consumption cycles. For example, during daytime hours, factories operate at full capacity, in the evening, families engage in cooking and household activities, and during nighttime hours, most individuals do not consume energy at significant levels. These consumption patterns are further influenced by whether it is a weekday or weekend. Therefore, to capture these temporal dependencies, time-varying variables are constructed to enable the LSTMs to associate and learn from these cyclical patterns.

The temporal feature engineering approach incorporates both linear and cyclic time representations to capture the multi-scale temporal dynamics inherent in carbon emissions data \parencite{hyndman2021}. Linear time features include hours since start, days since start, and weeks since start, which represent the elapsed time since the beginning of the dataset in different units, enabling the models to capture long-term trends and drift patterns. Additionally, year and year-month variables provide yearly and monthly progression information to account for long-term trends and seasonal variations at the annual scale.

To effectively represent the inherently cyclical nature of temporal patterns, sine and cosine transformations are employed for various time periods. The hourly cycle is captured through hourly sine and cosine variables, which encode the 24-hour daily rhythm that reflects diurnal energy consumption patterns, such as peak demand during business hours and minimal usage during overnight periods. Weekly patterns are represented by daily sine and cosine variables, capturing the distinction between weekday and weekend consumption behaviors. Monthly cycles are encoded via monthly sine and cosine variables to account for seasonal variations in energy demand, such as increased heating requirements during winter months. Finally, quarterly sine and cosine variables capture quarterly business and economic cycles that may influence industrial energy consumption patterns \parencite{yasmeen2022}. This cyclic feature engineering approach ensures that temporal relationships are preserved across period boundaries, allowing the LSTMs to recognize that the end of one cycle naturally transitions to the beginning of the next.

The final dataset has been constructed incorporating both exogenous variables and time-dependent features. For a comprehensive overview of the complete dataset and the associated variable names, see \autorefapdx{apdx:final-dataset}.

\subsection{Data Preprocessing}

Prior to model training, the assembled dataset underwent several preprocessing steps to ensure data quality and compatibility with the LSTM architectures. The data provided by EQ contained no missing values, eliminating the need for imputation procedures. This data completeness is attributable to EQ's proprietary synthetic data generation methods. The target variable was converted from kilograms to tonnes of \cotwoe to enhance interpretability and enable comparison with findings reported in the existing literature.

All observations were ordered chronologically from oldest to newest to maintain the temporal sequence essential for time series modeling. The dataset was then split into training, validation, and test sets using a 70/15/15 ratio, with validation and test periods following the training period chronologically. This temporal split prevents data leakage that could occur with random sampling approaches by ensuring that models only utilize historical information available at prediction time, closely mimicking operational forecasting scenarios \parencite{cerqueira2020,tashman2000}. The test set remains strictly held out throughout the entire model development process and is used exclusively for final performance evaluation.

To address the varying scales and units across the feature set, all variables underwent min-max normalization, scaling each feature to a range between zero and one. This normalization step is particularly important for neural network training, as it ensures that variables with larger absolute values do not dominate the learning process and helps maintain stable gradient flows during backpropagation \parencite{goodfellow2016}. The scaling parameters were fitted exclusively on the training set and subsequently applied to the validation and test sets to prevent information leakage from future observations into the model training process.

Outlier detection and removal procedures were deliberately omitted to preserve the integrity of the temporal sequence and maintain realistic operating conditions within the dataset. In the context of electrical grid operations, extreme values in carbon emissions often represent legitimate operational scenarios such as unexpected plant outages, grid emergencies, or periods of low renewable generation that require rapid deployment of backup thermal capacity. Removing such observations would eliminate precisely the challenging conditions that the forecasting models must learn to handle in real-world deployment. Furthermore, the LSTM architecture's gating mechanisms provide inherent robustness to occasional extreme values while preserving the temporal continuity essential for learning sequential patterns \parencite{goodfellow2016}.

\subsection{LSTM Model Architectures}
\label{subsec:lstm-model-architecture}

This thesis focuses on short-term CO2 emission forecasting with a target horizon of 1--24 hours, a timeframe that aligns with the practical use cases outlined in the introduction \parencite{entsoe2022,futurebridge}. To systematically evaluate the effectiveness of LSTMs for carbon emission prediction, two distinct architectures are developed. The first architecture targets a simpler 1-hour ahead forecasting task (output \(\hat{y}^{(t+1)}\)), while the second addresses the primary research objective of 24-hour ahead prediction (output \(\mathbf{\hat{y}}^{(t+1:t+24)}\)). This approach allows to first gain understanding and insights with LSTM performance on this specific forecasting problem through a less complex prediction task. The learnings acquired from the first model can then inform the design decisions for the 24-hour architecture. However, each architecture will be developed independently, with hyperparameters such as LSTM units, input sequence length, learning rate, and batch size explored separately to optimize performance for their respective forecasting horizons.

Beyond hyperparameters, several fundamental architectural and training decisions must be established for the LSTM implementations. These decisions can be categorized into those that can be determined prior to model training based on theoretical considerations and problem characteristics, and those that require empirical exploration through the training process. This subsection first addresses the former category, establishing the core training configuration choices that form the foundation of the LSTM architectures, before outlining a systematic framework for exploring the empirical architectural decisions that require experimental evaluation.

Mean squared error (MSE), as presented in \autoref{sec:theory}, is employed as the loss function for this regression task, providing differentiable gradients for stable LSTM optimization \parencite{goodfellow2016}. The Adam optimizer is employed for backpropagation due to its demonstrated effectiveness with time series data \parencite{makinde2024}, adaptive learning rates for each parameter, and robust convergence properties suitable for complex temporal dependencies in grid data \parencite{chang2018}.

Two callback mechanisms enhance training stability: early stopping (patience of five epochs) prevents overfitting by terminating training when validation loss stagnates, while a learning rate scheduler reduces the rate by 0.5 after three epochs without improvement (minimum 0.00001). This addresses LSTM training challenges where fixed learning rates prevent fine-tuning, allowing models to train until convergence rather than for a predetermined epoch count \parencite{goodfellow2016}.

The 1-hour and 24-hour forecasting tasks employ distinct architectural approaches tailored to their respective prediction complexities and horizons. For the 1-hour ahead forecasting task, a single-layer LSTM configuration is implemented. This design choice is justified by the relatively straightforward nature of single-step prediction, where the model outputs a scalar value (\(\hat{y}^{(t+1)}\)) representing carbon emissions one hour into the future. The single-layer architecture provides sufficient modeling capacity for this prediction horizon while minimizing computational complexity and reducing overfitting risk \parencite{greff2017}. In contrast, the 24-hour ahead forecasting task employs an encoder-decoder architecture to address the inherent complexity of multi-step sequence prediction. This design choice is motivated by several factors specific to the 24-hour forecasting challenge. First, the encoder-decoder architecture naturally accommodates the sequence-to-sequence nature of generating 24 consecutive hourly predictions (\(\mathbf{\hat{y}}^{(t+1:t+24)}\)), where the encoder processes historical time series data to create a compressed representation, and the decoder generates the multi-step forecast sequence \parencite{sutskever2014}. Second, this architecture effectively handles the integration of both historical features and future exogenous variables, such as consumption forecasts, which are available at prediction time and crucial for accurate long-term forecasting. Finally, the encoder-decoder architecture addresses the increased complexity of multi-step ahead prediction by providing dedicated components for feature extraction and sequence generation, enabling the model to capture both long-term temporal dependencies and the evolving patterns across the 24-hour prediction horizon \parencite{he2024}.

The hyperparameter optimization follows a coordinate descent approach designed to efficiently explore the hyperparameter space within computational constraints. The procedure begins by randomly selecting initial values from the predefined ranges specified in \autoref{tab:hyperparameter-ranges-lstm} to establish a starting configuration \parencite{bergstra2018}. Subsequently, each hyperparameter is optimized individually while holding others constant, following the sequence: number of LSTM units, input sequence length, learning rate, and batch size.

\begin{table}[ht]
  \centering
  \begin{tabular}{lc}
    \hline
    \textbf{Hyperparameter} & \textbf{Values}          \\ \hline
    Number of LSTM units    & 16, 32, 64, 128, 256     \\
    Input sequence length   & 24, 48, 72, 96           \\
    Learning rate           & .01, .001, .0001, .00001 \\
    Batch size              & 16, 32, 64, 128, 256     \\ \hline
  \end{tabular}
  \caption{Hyperparameter Values for LSTM Models Exploration}
  \label{tab:hyperparameter-ranges-lstm}
\end{table}

For each hyperparameter, the optimization explores adjacent values in a hill-climbing manner. Starting from the current value, the procedure evaluates neighboring options (e.g., if currently using 32 LSTM units, testing both 16 and 64 units) and selects the configuration that yields the lowest validation RMSE. If improvement is observed, the search continues in that direction until no further improvement is achieved. If no improvement is found in either direction, the current value is retained as optimal for that hyperparameter. This process ensures systematic exploration while avoiding exhaustive grid search.

Following the individual hyperparameter optimization, a final validation phase conducts random perturbations across all hyperparameters simultaneously to identify potential hyperparameter interactions that the coordinate descent approach might miss. This two-stage procedure balances computational efficiency with thorough exploration, ensuring robust hyperparameter selection for both the 1-hour and 24-hour forecasting architectures.

The combination of theoretically justified design choices and systematic manual hyperparameter tuning provides a robust foundation for developing optimal LSTM architectures tailored to the specific characteristics of carbon emission forecasting in the DK1 grid.

\subsection{Detailed Encoder-Decoder Architecture}

The encoder-decoder architecture for 24-hour forecasting is specifically designed to leverage both historical time series patterns and future exogenous variable forecasts available at prediction time. The encoder processes historical observations \((\mathbf{x}^{(t-L+1)}, \mathbf{x}^{(t-L+2)}, \ldots, \mathbf{x}^{(t)})\) to create compressed representations of temporal patterns, while the decoder generates the 24-hour prediction sequence \((\mathbf{\hat{y}}^{(t+1:t+24)})\) using both encoder output and future exogenous variable forecasts \(\mathbf{x}^{(t+h)}\) available at prediction time. This design directly addresses the merit-order by incorporating forecasted renewable generation, consumption, and market variables that determine the expected dispatch order and resulting carbon emissions. The decoder architecture mirrors the encoder in terms of LSTM units but includes an additional dense layer with linear activation to produce the carbon emission prediction at each time step.

This architectural choice is particularly well-suited to the DK1 forecasting task because EQ provides 24-hour ahead forecasts for all explanatory variables at prediction time, simulating the realistic operational scenario where grid operators have access to day-ahead explanatory variables when making carbon emission predictions.

\subsection{Baseline Models and Evaluation}

To assess the performance of the LSTM architectures, two baseline models are employed for comparative evaluation: a naive persistence model and an Autoregressive Integrated Moving Average (ARIMA) model. Each baseline is developed for both the 1-hour and 24-hour forecasting horizons. The naive persistence model represents the simplest forecasting approach, where future values are assumed to equal the most recent observation. For the 1-hour forecasting task, this translates to \(\hat{y}^{(t+1)} = y^{(t)}\), where the predicted carbon emissions one hour ahead equal the current observed value. For the 24-hour forecasting horizon, the naive model extends this logic across the entire prediction sequence, such that \(\hat{y}^{(t+h)} = y^{(t)}\) for \(h \in \{1, 2, \ldots, 24\}\), meaning all future hourly predictions are set equal to the current carbon emission level.

The ARIMA model serves as a traditional time series forecasting baseline that captures linear temporal dependencies through autoregressive, differencing, and moving average components. To ensure optimal ARIMA performance, a systematic grid search is conducted across all combinations of parameters: autoregressive order (\(p\)) from 0 to 5, differencing order (\(d\)) from 0 to 2, moving average order (\(q\)) from 0 to 5, and trend specifications including none, constant, and linear trends. The best-performing ARIMA configuration is selected based on out-of-sample validation RMSE. This results in six distinct models for comparison: two LSTM architectures, two naive baselines, and two optimally-configured ARIMA models, each tailored to their respective forecasting horizons.

These baseline models provide complementary benchmarks for evaluating the LSTMs. The naive models serve as a fundamental threshold that any forecasting model must surpass to demonstrate value, while the ARIMA models represent the established statistical approach capable of capturing linear temporal dependencies and trends. Together, they create a range from minimal sophistication to traditional time series modeling, enabling assessment of whether the LSTM architectures can justify their computational complexity by capturing the complex nonlinear relationships between renewable generation, grid dynamics, and carbon emissions that simpler linear approaches cannot effectively model.

The performance of all forecasting models is evaluated using Root Mean Squared Error (RMSE) as the primary evaluation metric, calculated as:

\[
  RMSE = \sqrt{\frac{1}{m}\sum_{i=1}^{m}\|\mathbf{\hat{y}}^{(i)} - \mathbf{y}^{(i)}\|_2^2}
\]

where \(m\) represents the number of test examples. RMSE serves as the primary comparison metric for determining LSTM superiority over baseline models because its quadratic penalty structure emphasizes larger prediction errors, aligning with the operational reality that substantial forecasting errors in carbon emissions carry disproportionately severe consequences for grid planning and regulatory compliance. Additionally, RMSE maintains the same units as the target variable, facilitating intuitive interpretation of forecasting accuracy.
Mean Absolute Error (MAE) provides complementary evaluation insight, defined as:

\[
  MAE = \frac{1}{m}\sum_{i=1}^{m}\|\mathbf{\hat{y}}^{(i)} - \mathbf{y}^{(i)}\|_1
\]

MAE offers equal weight to all prediction errors and reduced sensitivity to outliers, providing additional understanding of typical forecasting performance under normal operating conditions.

To ensure that RMSE improvements achieved by LSTM models represent statistically significant advances over baseline approaches rather than random variation, the Diebold-Mariano test is employed for pairwise forecast accuracy comparisons. This test is specifically designed for comparing predictive accuracy of competing forecasting models and accounts for the potential correlation in forecast errors that arises from using the same underlying dataset. The test statistic is calculated based on the RMSE loss differential series between competing models, where the null hypothesis assumes equal predictive accuracy. The test is applied to compare LSTM performance against each baseline model (naive persistence and ARIMA) for both forecasting horizons, using a two-sided test at a significance level of \(\alpha = 0.05\). Rejection of the null hypothesis indicates that the observed RMSE differences are statistically significant rather than attributable to random variation in the test set \parencite{diebold1995}.

This statistical validation is particularly important for carbon emission forecasting applications where model deployment decisions must be justified based on demonstrable performance improvements that exceed measurement uncertainty and natural variability in grid operations. The combination of RMSE-based performance comparison and Diebold-Mariano statistical testing provides robust assessment of whether the increased computational complexity of LSTM models translates into meaningful and statistically significant improvements in carbon emission prediction accuracy.

\subsection{Feature Importance Analysis}

To validate the relevance of the selected explanatory variables and feature engineering choices, permutation importance analysis is conducted on the trained 24-hour LSTM model. This analysis provides empirical insight into which features contribute most significantly to the model's predictive performance and confirms whether the theoretical justifications for variable selection align with the model's actual learning behavior \parencite{altmann2010}.

Permutation importance operates by systematically shuffling the values of each feature in the validation set while keeping all other features unchanged, then measuring the resulting degradation in model performance. The importance score for each feature is calculated as the difference between the baseline validation RMSE and the RMSE obtained after permuting that feature's values. For each feature, the permutation process is repeated 10 times with different random seeds to ensure robust importance estimates \parencite{altmann2010}. This approach is particularly suitable for LSTM architectures because it treats the model as a black box, avoiding the need to interpret complex internal states while naturally handling temporal dependencies in the input sequences.

The analysis focuses exclusively on the 24-hour LSTM model due to its primary practical relevance for the operational use cases outlined in the introduction and its more comprehensive feature set that includes both historical patterns and future exogenous variable forecasts. The analysis encompasses both the original exogenous variables collected from EQ and the engineered temporal features.

This permutation importance analysis serves three key validation purposes: confirming whether variables identified as conceptually important based on merit-order principles actually contribute to model performance, revealing the relative importance of different feature categories (supply-side versus demand-side variables), and assessing the contribution of engineered time-based patterns relative to domain-specific grid variables. These insights enhance the interpretability of the LSTM model and provide confidence in how the architecture leverages available information for carbon emission forecasting in the DK1 grid.

\subsection{Implementation Details}

The full implementation from data retrieval to modeling has been done using Python. All the source code is provided in a zip-file as additional material. For an overview of the different components see \autorefapdx{apdx:code-overview}. Most notably, data was extracted via \href{https://energyquantified-python.readthedocs.io/}{EQs Time Series API }, transformed using \href{https://pola.rs/}{Polars}, and finally modelled using \href{https://www.tensorflow.org/}{TensorFlow} - one of the most popular libraries for working with neural networks.

\subsection{Limitations}

While this methodology provides a focused approach to carbon emission forecasting in DK1, several design choices impose boundaries on the scope and generalizability of findings.

The exclusive focus on LSTM networks excludes evaluation of other promising sequential architectures identified in the literature. Alternatives such as gated recurrent units (GRUs), temporal convolutional networks (TCNs), and Transformer-based encoders offer different approaches to modeling temporal dependencies. This architectural limitation prevents direct comparison of LSTM performance against these potentially suitable alternatives for carbon emission forecasting.

Despite substantial literature evidence that hybrid statistical-machine learning approaches consistently outperform standalone models, this thesis focuses exclusively on pure LSTM architectures compared to traditional baselines. The literature reviewed in \autoref{sec:lit} demonstrates that ARIMA-LSTM combinations frequently achieve superior accuracy to either method individually, yet this methodology deliberately excludes such hybrid approaches to maintain focus on evaluating standalone LSTM capabilities. Similarly, the comparative evaluation is limited to naive persistence and ARIMA models, excluding other machine learning baselines such as random forests, gradient boosting, or support vector regression that the literature suggests can be competitive for energy forecasting tasks.

The DK1-focused dataset, while addressing an identified research gap, may limit generalizability to other renewable-dominated grids with different operational characteristics, market structures, or renewable penetration levels. The unique wind-heavy profile of DK1 may not represent patterns applicable to other regional contexts. Additionally, the reliance on EQ's proprietary gap-filling methods introduces potential unknown biases into the dataset. While this ensures completeness, the synthetic data generation process may alter natural patterns in ways that could influence model learning and evaluation.

The 70/15/15 temporal split allocates the majority of historical observations to training, with validation and test sets representing the most recent periods of the dataset. While this split maintains chronological ordering essential for time series evaluation, the test period's position at the end of the dataset means that final performance evaluation reflects the model's ability to predict carbon emissions during the most recent operational period. This recent period may exhibit different patterns from earlier historical periods due to evolving grid infrastructure, policy changes, or market dynamics, potentially limiting the representation of seasonal variations across the full dataset timeframe. This temporal limitation is inherent to time series forecasting evaluation but should be considered when interpreting the generalizability of performance results to future operational periods that may extend beyond the characteristics captured in the training data.
